import os
import sys
import numpy as np
import torch
import torch.nn.functional as F

now_dir = os.getcwd()
sys.path.append(now_dir)

from src.constants import *
from src.spec import MelSpectrogram


class HPARMVPE:
    """
    A predictor for fundamental frequency (F0) based on the HPA-RMVPE model.

    Args:
        model_path (str): Path to the HPA-RMVPE model file.
        device (str, optional): Device to use for computation. Defaults to "cpu".
        hop_length (int, optional): Hop length for mel spectrogram. Defaults to 160.
        n_gru (int, optional): Number of GRU layers. Defaults to 1.
        in_channels (int, optional): Input channels. Defaults to 1.
        en_out_channels (int, optional): Encoder output channels. Defaults to 16.
    """

    def __init__(
        self,
        model_path: str,
        device: str | torch.device = "cpu",
        hop_length: int = 160,
        n_gru: int = 1,
        in_channels: int = 1,
        en_out_channels: int = 16,
    ):
        from src.model import E2E0

        device = device or ("cuda" if torch.cuda.is_available() else "cpu")

        model = E2E0(n_gru, in_channels, en_out_channels)
        model.load_state_dict(torch.load(model_path, map_location="cpu", weights_only=True))
        model.eval()
        self.model = model.to(device).float()

        self.device = device
        self.mel_extractor = MelSpectrogram(N_MELS, SAMPLE_RATE, WINDOW_LENGTH, hop_length, None, MEL_FMIN, MEL_FMAX).to(device)
        cents_mapping = 20 * np.arange(N_CLASS) + CONST
        self.cents_mapping = np.pad(cents_mapping, (4, 4))

    def mel2hidden(self, mel, chunk_size=SAMPLE_RATE * 2):
        """
        Converts Mel-spectrogram features to hidden representation.

        Args:
            mel (torch.Tensor): Mel-spectrogram features.
            chunk_size (int): Size of chunks for processing.
        """
        with torch.no_grad():
            n_frames = mel.shape[-1]
            mel = F.pad(
                mel, (0, 32 * ((n_frames - 1) // 32 + 1) - n_frames), mode="reflect"
            )

            output_chunks = []
            pad_frames = mel.shape[-1]
            for start in range(0, pad_frames, chunk_size):
                end = min(start + chunk_size, pad_frames)
                mel_chunk = mel[..., start:end]
                assert (
                    mel_chunk.shape[-1] % 32 == 0
                ), "chunk_size must be divisible by 32"
                out_chunk = self.model(mel_chunk.float())
                output_chunks.append(out_chunk)

            hidden = torch.cat(output_chunks, dim=1)
        return hidden[:, :n_frames]

    def decode(self, hidden, thred=0.03):
        """
        Decodes hidden representation to F0.

        Args:
            hidden (np.ndarray): Hidden representation.
            thred (float, optional): Threshold for salience. Defaults to 0.03.
        """
        cents_pred = self.to_local_average_cents(hidden, thred=thred)
        f0 = 10 * (2 ** (cents_pred / 1200))
        f0[f0 == 10] = 0
        return f0

    def infer_from_audio(self, audio, thred=0.03):
        """
        Infers F0 from audio.

        Args:
            audio (np.ndarray): Audio signal.
            thred (float, optional): Threshold for salience. Defaults to 0.03.
        """
        audio = torch.from_numpy(audio).float().to(self.device).unsqueeze(0)
        mel = self.mel_extractor(audio, center=True)
        del audio
        with torch.no_grad():
            torch.cuda.empty_cache()
        hidden = self.mel2hidden(mel)
        hidden = hidden.squeeze(0).cpu().numpy()
        f0 = self.decode(hidden, thred=thred)
        return f0

    def infer_from_audio_with_pitch(self, audio, thred=0.03, f0_min=50, f0_max=1100):
        """
        Infers F0 from audio with pitch range filtering.

        Args:
            audio (np.ndarray): Audio signal.
            thred (float, optional): Threshold for salience. Defaults to 0.03.
            f0_min (float, int, optional): Minimum F0 threshold.
            f0_max (float, int, optional): Maximum F0 threshold.
        """
        f0 = self.infer_from_audio(audio, thred)
        f0[(f0 < f0_min) | (f0 > f0_max)] = 0
        return f0

    def to_local_average_cents(self, salience, thred=0.05):
        """
        Converts salience to local average cents.

        Args:
            salience (np.ndarray): Salience values.
            thred (float, optional): Threshold for salience. Defaults to 0.05.
        """
        center = np.argmax(salience, axis=1)
        salience = np.pad(salience, ((0, 0), (4, 4)))
        center += 4
        todo_salience = []
        todo_cents_mapping = []
        starts = center - 4
        ends = center + 5
        for idx in range(salience.shape[0]):
            todo_salience.append(salience[:, starts[idx]:ends[idx]][idx])
            todo_cents_mapping.append(self.cents_mapping[starts[idx]:ends[idx]])
        todo_salience = np.array(todo_salience)
        todo_cents_mapping = np.array(todo_cents_mapping)
        product_sum = np.sum(todo_salience * todo_cents_mapping, 1)
        weight_sum = np.sum(todo_salience, 1)
        devided = product_sum / weight_sum
        maxx = np.max(salience, axis=1)
        devided[maxx <= thred] = 0
        return devided


if __name__ == "__main__":
    import librosa
    import matplotlib.pyplot as plt

    model = HPARMVPE("hpa-rmvpe.pt", device="cpu")
    y, sr = librosa.load("voice.mp3", sr=SAMPLE_RATE, mono=True)

    f0 = model.infer_from_audio(y)

    print(f0.shape)

    plt.figure(figsize=(10, 4))
    plt.plot(f0)
    plt.title("RMVPE")
    plt.xlabel("Time")
    plt.ylabel("Frequency")
    plt.savefig("f0-rmvpe.png")
    plt.show()
